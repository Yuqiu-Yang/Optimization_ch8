---
title: "Introduction to Optimization and Nonlinear Equations"
author: "Zeyu Lu & Yuqiu Yang"
header-includes:
  - \usepackage{subfig}
output: 
  beamer_presentation:
    theme: "Berkeley"
    colortheme: "dolphin"
    fonttheme: "structurebold"
    toc: true
    slide_level: 2
    includes:
      in_header: header_pagenrs.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(latex2exp)
library(knitr)
library(kableExtra)
library(gridExtra)
library(grid)
```

# Safe Univariate Methods:

## Optimization Problem: Definition
In mathematics, computer science and economics, an optimization problem is the problem of finding the best solution from all feasible solutions.--wiki\newline\newline

Such as finding the maximum/minimum value for a certain function that is defined on a discrete set/continuum\newline\newline

## Optimization Problem: Examples
```{r, fig.align='center', fig.height=11, echo=FALSE}
x<-seq(0,10,0.001)
y1<-function(x){(x-5)^2+25}
y2<-function(x){-(x-3)^2+18}
y3<-function(x){abs((x-5)^3)}
y4<-function(x){2*abs(x-5)+10}
par(mfrow=c(2,2)) 
plot(x,y1(x),type='l',col='blue',main=expression(f(x)==(x-5)^2+25))
plot(x,y2(x),type='l',col='green',main=expression(f(x)==-(x-3)^2+18))
plot(x,y3(x),type='l',col='red',main=expression(f(x)==(x-5)^3))
plot(x,y4(x),type='l',col='black',main=expression(f(x)==2*abs(x-5)+10))
```

## Optimization Problem: Assumptions and efficiency
(1)Assumptions needed for each methods?\newline\newline
(2)which method is less restrictive?\newline\newline
(3)How to evaluate the efficiency of a method?\newline\newline

## Lattice Search
Finding the maximum of a unimodal function $f$ on a discrete set of points ${1,2,...,m}$ a lattice\linebreak\linebreak
Unimodal function, A function $f(x)$ is said to be unimodal function if for some value $m$ is monotonically increasing for $x\leq m$ and monotonically decreasing for $x\geq m$.\linebreak\linebreak

## Lattice Search: Unimodal Function on Discrete Points
Graph for $f(x)=-4x^2+24x$, this is a unimodal function.
```{r, fig.align='center', out.width='70%', echo=FALSE}
x<-seq(0,6,0.5)
y<-function(x){-4*(x-3)^2+36}
plot(x,y(x),xlim=c(0,6),ylim=c(0,40),axes=F,ann=F)
points(3,y(3),pch=16)
box()
title(main=expression(paste('Unimodel Function ',f(x)==-4*x^2+24*x)))
axis(1,at=0:10)
axis(2,at=seq(0,40,5),las=2)
abline(v=3,lty=2,lwd=2)
text(3.2,5,'x=3')
arrows(3.5,30,3.2,34,length=0.1)
text(3.7,26,'Max at (3,36)')
arrows(0.5,20,1.2,30,length=0.1)
arrows(4.7,30,5.5,20,length=0.1)
text(0.7,28,'increasing',srt=50)
text(5.3,27,'decreasing',srt=-44)
```
\noindent for $x<3$, function value $f(x)$ is monotonically increasing, and for $x\geq3$, $f(x)$ is monotonically decreasing.\newline

## Lattice Search: Optimal Strategy
(i)Finding good end strategies for finding the mode on a small set of points\linebreak\linebreak

(ii)Employing backwards induction to start with the right strategy to match the optimal ending\linebreak\linebreak

Basically, optimal strategy means the fewest evaluations of the function $f$ that will solve all problems that meet the specifications, here is any strictly unimodel function.

## Lattice Search: How to choose points to evaluate?
```{r, fig.align='center', out.width='70%', echo=FALSE}
x<-seq(0,6,0.5)
y<-function(x){-4*(x-3)^2+36}
plot(x,y(x),xlim=c(0,6),ylim=c(0,40),axes=F,ann=F)
points(4,y(4),pch=16)
arrows(4.7,33,4.2,32.2,length=0.1)
text(5,35,'(4,f(4))')
points(1,y(1),pch=16)
arrows(1.7,18,1.2,19.5,length = 0.1)
text(2,17,'(1,f(1))')
points(0.5,y(0.5),pch=4)
points(0,y(0),pch=4)
box()
title(main=expression(paste('Unimodel Function ',f(x)==-4*x^2+24*x)))
axis(1,at=0:10)
axis(2,at=seq(0,40,5),las=2)
abline(v=1,lty=2,lwd=2)
polygon(c(1,1,-5,-5),c(-5,42,42,-5),density = 10)
```
If $f(4)>f(1)$, then we immediately discard the points that are less than $x=1$, otherwise it will
violate the assumption of unimodality\newline

## Lattice Search: How to choose points to evaluate?
```{r, fig.align='center', out.width='70%', echo=FALSE}
x<-seq(0,6,0.5)
y<-function(x){-4*(x-3)^2+36}
plot(x,y(x),xlim=c(0,6),ylim=c(0,40),axes=F,ann=F)
points(4,y(4),pch=16)
points(0.5,25,pch=16)
points(0,30,pch=16)
arrows(4.7,33,4.2,32.2,length=0.1)
text(5,35,'(4,f(4))')
points(1,y(1),pch=16)
arrows(1.7,18,1.2,19.5,length = 0.1)
arrows(1,35,0.6,26,length=0.1)
text(1.3,37,'violating assumption')
text(2,17,'(1,f(1))')
points(0.5,y(0.5),pch=4)
points(0,y(0),pch=4)
box()
title(main=expression(paste('Unimodel Function ',f(x)==-4*x^2+24*x)))
axis(1,at=0:10)
axis(2,at=seq(0,40,5),las=2)
```
If $f(4)>f(1)$, then we immediately drop the points that are less than $x=1$, otherwise it will
violate the assumption of unimodel function\newline


## Lattice Search: How to choose points to evaluate?
If the points are too far away from each other, then only a few portion of points can be discarded in one step, which decreases the efficiency.\newline\newline

If the two points are close to center of the domain, it's difficult to reuse any of them in the next step, and even we hope to reuse them in the following steps.\newline\newline

## Lattice Search: How to choose points to evaluate?
```{r, fig.align='center', out.width='70%', echo=FALSE}
x<-seq(0,6,0.3)
y<-function(x){-4*(x-3)^2+36}
par(mfrow=c(1,2))
plot(x,y(x),xlim=c(0,6),ylim=c(0,40),axes=F,ann=F)
points(2.7,y(2.7),pch=16)
points(3,y(3),pch=16)
box()
title(main=expression(f(x)==-4*x^2+24*x))
axis(1,at=0:10)
axis(2,at=seq(0,40,5),las=2)
polygon(c(2.7,2.7,-1,-1),c(-10,100,100,-10),density=10)
arrows(3.5,25,3.1,35,length=0.1)
text(3.7,22,'Hard to reuse')
plot(x,y(x),xlim=c(0,6),ylim=c(0,40),axes=F,ann=F)
points(0.3,y(0.3),pch=16)
points(5.4,y(5.4),pch=16)
box()
title(main=expression(f(x)==-4*x^2+24*x))
axis(1,at=0:10)
axis(2,at=seq(0,40,5),las=2)
polygon(c(0.3,0.3,-1,-1),c(-10,100,100,-10),density=10)
```

## Lattice Search: Fibonacci Numbers
The optimal strategy is by applying Fibonacci numbers\newline\newline
$F_n=\{1,2,3,5,8,13,...,F_n=F_{n-1}+F_{n-2}\}$\newline\newline
Suppose we have a set of discrete points$\{1,2,3,...,m=F_n-1\}$, and we begin the searching by evaluating at the points$F_{n-2}$and$F_{n-1}$.\newline\newline
If $f(F_{n-2})<f(F_{n-1})$, then the sub-problem is $\{F_{n-2}+1,...,F_n-1\}$ with $f(F_{n-1})$ has already been evaluated, thus a problem with $F_{n}-1$ elements needs $n-1$ evaluations to solve.

## Lattice Search: Fibonacci Numbers
$F_n=\{1,2,3,5,8,13\}$
```{r, fig.align='center', out.width='70%', echo=FALSE}
x<-seq(0,5.5,0.5)
y<-function(x){-4*(x-3)^2+36}
plot(x,y(x),xlim=c(0,5.5),ylim=c(0,40),axes=F,ann=F)
points(2,y(2),pch=16)
points(3.5,y(3.5),pch=16)
points(4.5,y(4.5),pch=16)
box()
title(main=expression(paste('Unimodel Function ',f(x)==-4*x^2+24*x)),sub=expression(m==F[6]-1))
axis(1,at=0:10)
axis(2,at=seq(0,40,5),las=2)
abline(v=2,lty=2,lwd=2)
abline(v=4.5,lty=2,lwd=2)
arrows(2.3,25,2.1,30,length=0.1)
text(2.5,22,expression(F[4]==5))
arrows(3.5,28,3.5,33,length=0.1)
text(3.5,25,expression(F[5]==8))
arrows(4.2,20,4.4,25,length=0.1)
text(4.2,18,'evaluated at 2nd step')
text(1,35,'dropped at 1st step')
text(5,5,'dropped at 2nd step')
polygon(c(2,2,-5,-5),c(-5,42,42,-5),density = 10)
polygon(c(4.5,4.5,7,7),c(-5,42,42,-5),density = 10)
```
For $m=F_{6}-1=12$, 5 evaluations are enough to reach the maximal value.\newline after each step, we got a sub-problem with $F_{n-1}-1$ points.\newline

## Lattice Search: Details
(i)If the values of the function are the same at $F_{n-2}$ and $F_{n-1}$, the mode must be between the two points according to our assumption, then it doesn't matter which part is discarded.\newline\newline
(ii)If the number of points m is not one fewer than a Fibonacci number, then add some points at one side. where the value of additional points is $-\infty$.\newline\newline

## Golden Section Search
A more common problem is searching for the maximum on a continuum. so without losing generality, we set the interval (0,1).\newline\newline
Divided the interval and use lattice search by placing m points in the interval, the set is\newline\newline $\{0,\frac{1}{m-1},\frac{2}{m-1},...,1\}$.\newline\newline
First two points\newline\newline
$lim\frac{F_{n-2}-1}{F_n-1}$ and $lim\frac{F_{n-1}-1}{F_n-1}$

## Golden Section Search
And since we knew that the lattice search is defined by the first two evaluations, let m goes to infinity, then\newline\newline
Set$lim\frac{F_{n-1}}{F_n}=\phi$, so that $lim\frac{F_{n}}{F_{n-1}}=\frac{1}{\phi}=lim\frac{F_{n-2}+F_{n-1}}{F_{n-1}}=\phi+1$\newline\newline
$\phi^2+\phi+1=0,\phi=\frac{\sqrt{5}-1}{2}\approx.0618$\newline\newline
Which also known as the golden ratio. thus the starting points of the search are\newline\newline
$X_1=\phi^2\approx0.382,X_2=\phi\approx0.618$\newline\newline
The limit of the lattice search is called the golden section search.

## Golden Section Search
```{r, fig.align='center', out.width='70%', echo=FALSE}
x<-seq(0,1,0.001)
y<-function(x){-(x-.4)^2+.36}
plot(x,y(x),type='l',xlim=c(0,1),ylim=c(0,0.4),lwd=2,axes=F,ann=F)
points(0.382,y(0.382),pch=16)
points(0.618,y(0.618),pch=16)
points(0.236,y(0.236),pch=16)
box()
title(main=expression(paste('Unimodel Function',f(x)==-(x-0.4)^2+0.36)))
abline(v=0.382,lty=2)
abline(v=0.618,lty=2)
axis(1,at=seq(0,1,0.1))
axis(2,at=seq(0,0.4,0.05),las=2)
polygon(c(0.618,0.618,2,2),c(-1,1,1,-1),density=20)
arrows(0.55,0.25,0.6,0.3,length=0.1)
arrows(0.3,0.3,0.36,0.34,length=0.1)
arrows(0.13,0.35,0.2,0.34,length=0.1)
text(0.5,0.22,expression(phi==0.618))
text(0.25,0.27,expression(phi^2==0.382))
text(0.07,0.37,expression(phi^3==0.236))
text(0.07,0.33,'2nd step')
```
After the first step, the uncertainty of interval is $(0,\phi)$ and the point $\phi^2=0.382$ has already been evaluated. Noticed it is also the right point in the second step, which is very similar to lattice search, and we only need to evaluate one more point at $\phi^3\approx0.236$.\newline

## Bisection
Fibonacci search is less restrictive, since the derivative of the function $f$ doesn't need to exist, but suppose the derivative of the function $f$ is available, which would convert the problem from finding maximum of a unimodel function to finding the root of a monotone function $g$ on the same interval\newline

Suppose $g(x)$ is defined on interval $(a,b)$, and let$g(a)<0<g(b)$. with a single evaluation at $g(\frac{a+b}{2})$, the uncertainty of interval will be halved.

## Bisection
```{r, fig.align='center', out.width='70%', echo=FALSE}
x<-seq(0,5,0.001)
y<-function(x){-(x-5)^2+12}
plot(x,y(x),type='l',xlim=c(0,5),ylim=c(-18,18),ann=F,axes=F)
points(2.5,y(2.5),pch=16)
box()
axis(1,at=seq(0,5,1))
axis(2,at=seq(-15,15,3),las=1)
title(main=expression(paste('Monotone Function ',g(x)==-(x-5)^2+12)))
abline(h=0,lty=1,col='blue')
abline(v=2.5,lty=2,lwd=2)
arrows(2,9,2.4,6.5,length=0.1)
text(1.8,12,expression(paste(g(frac(5,2)),'>',0)))
polygon(c(2.5,2.5,10,10),c(-20,20,20,-20),density=20)
```
Thus, after the first step, we reset the right endpoint as $\frac{a+b}{2}$, and repeat this procedure to get the root.

## Comparison: Golden Section and Bisection
1. Golden Section:
  + Less restrictive, requiring only a strictly unimodel function.
  + It reduces the interval of uncertainty to $(0,\phi)$ in each iteration.

2. Bisection:
  + More restrictive, requiring the derivative exist and be available.
  + It halves the interval of uncertainty, that is $\frac{1}{2}$.

# Root finding 
## Newton's Method: Iteration Formula
The more common problem is finding a root for a single nonlinear equation $g(x)=0$.\newline\newline
For function $g$, set its derivative as $g'$, we have\newline\newline
$g_t(x)=g(x_{old})+g'(x_{old})(x-x_{old})$\newline\newline
$g_t(x)=0$ is at\newline\newline
$x_{new}=x_{old}-\frac{g(x_{old})}{g'(x_{old})}$\newline\newline
By using n, the iteration formula is:\newline\newline
$x_{n+1}=x_n-\frac{g(x_n)}{g'(x_n)}$\newline

## Newton's Method: Example
::: columns
:::: column
```{r, fig.align='center', fig.height=11, echo=FALSE}
x<-seq(0,5,0.001)
y<-function(x){-(x-5)^2+12}
y.dev<-function(x){-2*x+10}
plot(x,y(x),type='l',xlim=c(0,5),ylim=c(-18,18),ann=F,axes=F)
x=4
xn<-c()
gn<-c()
gn.d<-c()
for(i in 1:10){
  points(x,y(x),pch=16)
  xn[i]<-x
  gn[i]<-y(x)
  gn.d[i]<-y.dev(x)
  x<-x-y(x)/y.dev(x)
}
box()
axis(1,at=seq(0,5,1))
axis(2,at=seq(-15,15,3),las=1)
title(main=expression(paste('Monotone Function ',g(x)==-(x-5)^2+12)))
arrows(3.5,12,3.9,11.2,length=0.1)
arrows(2,-6,1.55,-0.3,length=0.1)
text(3.3,12,'Starting point',cex=1.5)
text(2.1,-8,'Points are gathering here after several steps',cex=1.5)
abline(h=0,lty=1,col='blue')
```
:::
:::: column
```{r}
######
test.data <- data.frame(matrix(NA, nrow = 10,
                             ncol = 3))
colnames(test.data) <- c("n", "xn", "gn")
test.data$n <- c(1:10)
test.data$xn <- xn
test.data$gn <- gn
knitr::kable(test.data, "simple")

```
::::
:::

## Newton's Method: Quadratic Convergence
If we denote the root by $c$ and the error at iteration $n$ by $e_n=x_n-c$\newline\newline
the relative error is $d_n=\frac{e_n}{c}=\frac{(x_n-c)}{c}$\newline\newline
By using Taylor expansion:\newline\newline
$g(c)=0=g(x_n)+(c-x_n)g'(x_n)+(c-x_n)^2\frac{g''(t)}{2}$\newline\newline
Where t lies between $x_n$ and $c$.\newline\newline

## Newton's Method: Quadratic Convergence
Noticed $e_{n+1}=x_{n+1}-c=x_n-\frac{g(x_n)}{g'(x_n)}-c$\newline\newline
Substitute into the equation.\newline\newline
$x_n-c-\frac{g(x_n)}{g'(x_n)}=(x_n-c)^2[\frac{g''(t)}{2g'(x_n)}]$\newline\newline
$e_{n+1}=e_n^2[\frac{g''(t)}{2g'(x_n)}]$\newline\newline
This expression reveals the quadratic convergence of Newton's Method.\newline

## Newton's Method: Steep or Flat?
::: columns
:::: column
```{r, fig.align='center', fig.height=11, echo=FALSE}
x<-seq(0,1,0.001)
y<-function(x){(x-0.5)^3+0.05}
y.d<-function(x){3*(x-0.5)^2}
plot(x,y(x),type='l',axes=F,ann=F)
x=0.51
xn<-c()
ratio<-c()
for(i in 1:10){
  points(x,y(x),pch=16)
  xn[i]<-x
  ratio[i]<-y(x)/y.d(x)
  x<-x-y(x)/y.d(x)
}
abline(h=0,lty=2,lwd=2,col='blue')
box()
title(main=expression(paste((x-0.5)^3," Domain: (0,1)",)))
axis(1,at=seq(0,1,0.2))
axis(2,at=seq(-0.2,0.2,0.05),las=2)
arrows(0.42,0.07,0.49,0.055,length=0.1)
text(0.4,0.08,'starting point x=0.51')
```
:::
:::: column
```{r}
######
test.data <- data.frame(matrix(NA, nrow = 10,
                             ncol = 3))
colnames(test.data) <- c("n", "xn", "ratio")
test.data$n <- c(1:10)
test.data$xn <- round(xn,3)
test.data$ratio <- round(ratio,3)
knitr::kable(test.data, "simple")
```
::::
:::
Noticed for a flat point, $g'(x)$ could be very small so that the next point may leap far away from the true root.\newline

## Newton's Method: Multivariate Case
For the multivariate case, finding the tangent approximation $\textbf{g}_t(\textbf{x})$ relies on the multivariate version.\newline\newline
$\textbf{g}_{t}(\textbf{x})=\textbf{g}(\textbf{x}_{old})+\textbf{J}_{\textbf{g}}(\textbf{x}-\textbf{x}_{old})$\newline\newline
$\textbf{x}_{new}=\textbf{x}_{old}-\textbf{J}_{\textbf{g}}(\textbf{x}_{old})^{-1}\textbf{g}(\textbf{x}_{old})$\newline\newline
Which has the same form as the univariate case, and the convergence rate is also quadratic.

## Newton's Method: Example
::: columns
:::: column
```{r, fig.align='center', fig.height=11, echo=FALSE}
x1<-x2<-seq(-2,2,0.01)
z<-outer(x1,x2,function(x1,x2){(x1^2+x2^2)/3})
contour(x1,x2,z,nlevels=8,levels=c(0.01,0.05,0.1,0.2,0.3,0.4,0.6,0.8))
title(main=expression(paste('Counter Plot for ',z==(x[1]^2+x[2]^2)/3)))
points(1.5,2,pch=16)
arrows(1.8,1,1.55,1.9,length=0.1)
text(1.8,0.8,'Starting Point')
x<-c(1.5,2)
z.func<-function(x1,x2)((x1^2+x2^2)/3)
z.d1<-function(x1){2*x1/3}
z.d2<-function(x2){2*x2/3}
x1.value<-rep(0,10)
x2.value<-rep(0,10)
z.vec<-rep(0,10)
for(i in 1:10){
  temp<-x
  z.value<-z.func(x[1],x[2])
  x1.value[i]<-temp[1]
  x2.value[i]<-temp[2]
  z.vec[i]<-z.value
  x[1]<-x[1]-z.value/z.d1(x[1])
  x[2]<-x[2]-z.value/z.d2(x[2])
  points(x[1],x[2],pch=16)
  arrows(temp[1],temp[2],x[1],x[2],length=0)
}
```
:::
:::: column
```{r}
######
test.data <- data.frame(matrix(NA, nrow = 10,
                             ncol = 3))
colnames(test.data) <- c("x1", "x2", "gn")
test.data$x1 <- round(x1.value,3)
test.data$x2 <- round(x2.value,3)
test.data$gn <- round(z.vec,3)
knitr::kable(test.data, "simple")
```
::::
:::

## Newton's Method: Pros and Cons
1. Pros:
  + Newton's method achieves the fastest rate of convergence(quadratic).

2. Cons:
  + The derivative function must be available, and finding it can be tedious or impossible.

## The Secant Method 
If $g'$ is hard or even impossible to find, we can approximate $g'(x) \approx \dfrac{g(x+h)-g(x)}{h}$. \linebreak
\linebreak
The iteration formula now becomes $x_{n+1} = x_{n} - g(x_n)\dfrac{x_{n} - x_{n-1}}{g(x_n) - g(x_{n-1})}$\linebreak
\linebreak
Notice two initial points are required instead of one like the Newton's method.

## The Secant Method: Geometrical Interpretation
Let $f(x) = sin(x) - (\dfrac{x}{2})^2$, $x_0 = -1.5$ and $x_{1} = 1.5$
```{r, fig.align='center', out.width='70%', echo=FALSE}
x <- seq(-2, 3, by = 0.01)
y <- function(x){sin(x) - x^2/4}
dy <- function(x) {cos(x) - x/2}
plot(x, y(x), type = 'l',
     main = "1st iteration Newton vs. Secant")
abline(h = 0, col = "cyan", lwd = 2)
x0 <- -1.5
b <- dy(x0)
a <- y(x0) - b * (x0)
newton_line <-  b * x + a
lines(x, newton_line, col = "salmon")
points(-a/b, 0, pch = 19, cex = 2, 
       col = "salmon")
x1 <- 1.5
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
points(-a/b, 0, pch = 19, cex = 2, 
       col = "darkorchid")
points(x0, y(x0))
points(x1, y(x1))
arrows(x0, -1, x0, y(x0), 
       angle = 20)
text(x0, -0.8, "x0 = -1.5")
arrows(x1, -0.1, x1, y(x1), 
       angle = 20)
text(x1, -0.3 , "x1 = 1.5")
legend("bottom", legend = c("Newton", "Secant",
                            "Newton 1st iter",
                            "Secant 1st iter"),
       col = c("salmon", "darkorchid"),
       lty = c(1,1,NA,NA),
       pch = c(NA,NA,19,19),
       bty = "n")
```
\noindent $x_{n + 1}$ is taken to be the abscissa of the point of intersection between the secant through $(x_{n-1}, f(x_{n-1}))$ and $(x_{n}, f(x_{n}))$ and the x-axis.

## The Secant Method: An Example
::: columns
:::: column
Let $f(x) = sin(x) - (\dfrac{x}{2})^2$\linebreak
$x_0 = -1.5$ and $x_{1} = 1.5$\linebreak 
```{r, echo=FALSE, fig.height=11}
x <- seq(-2, 3, by = 0.01)
y <- function(x){sin(x) - x^2/4}
plot(x, y(x), type = 'l',
     main = "Secant method")
abline(h = 0, col = "cyan", lwd = 2)
secant_method_ex <- function(FUN = y, x0, x1, e_tol = 1e-5)
{
  xn <- c(x0, x1)
  fn <- c(y(x0), y(x1))
  i <-  3
  err <- Inf
  while(err > e_tol)
  {
    b <- (y(xn[i-1]) - y(xn[i-2]))/(xn[i-1]-xn[i-2])
    a <- y(xn[i-2]) - b * (xn[i-2])
    xn <- c(xn, -a/b)
    fn <- c(fn, y(-a/b))
    err <- abs(y(-a/b))
    i <- i + 1
  }
  return(list(fn = fn,
              xn = xn,
              err = err))
}
temp <- secant_method_ex(y, -1.5,1.5)
points(temp$xn, temp$fn, pch = 19, cex =2.5,
       col = rgb(153/255, 50/255, 204/255, 
                 seq(0.1,1, length.out = length(temp$xn))))
```
::::
:::: column
```{r}
######
temp_df <- data.frame(matrix(NA, nrow = length(temp$fn),
                             ncol = 3))
colnames(temp_df) <- c("n", "xn", "fn")
temp_df$n <- 0:11

xns <- c(round(temp$xn, 4), "NA")
for(i in 1 : 12)
{
  xns[i] <- paste0("{", xns[i],",",xns[i+1],"}")
}
temp_df$xn <- xns[1:12]

temp_df$fn <- round(temp$fn, 4)
knitr::kable(temp_df, "simple")

```
::::
:::

## The Secant Method: Order of Convergence
Let the root the Secant Method approaches be $c$, then asymptotically, we will have \[|\epsilon_{n+1}| = C|\epsilon_n||\epsilon_{n-1}|,\] where $\epsilon_n = c - x_n$ and $C = |\dfrac{g''(c)}{2g'(c)}|$.\linebreak \linebreak
Based on this relationship, we can find $|\epsilon_n| = C|\epsilon_{n-1}|^{1.618}$\linebreak \linebreak
Since the exponent 1.618 lies between 1 (linear convergence) and 2 (quadratic convergence), the convergence rate of the Secant Method is called \emph{superlinear}.

## The Secant Method: Pros and Cons
1. Pros:
  + Superlinear convergence 
  + No need to evaluate derivatives 
2. Cons:
  + Convergence is not guaranteed 
  + Not well behaved when $g$ is relatively flat

::: columns
:::: column
```{r}
y <- function(x){x^2}
x <- seq(-3,3,by = 0.01)
plot(x, y(x), type = "l", main = "First iteration",
     ylab = TeX("$x^2$"))
x0 <- 1
x1 <- -2
points(x0, y(x0))
points(x1, y(x1))
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
arrows(x0, 4, x0, y(x0), 
       angle = 20)
text(x0, 4.2, "x0 = 1")
arrows(x1, 1.7, x1, y(x1), 
       angle = 20)
text(x1, 1.5 , "x1 = -2")
abline(h = 0, col = "cyan", lwd = 2)
points(-a/b, 0)
x2 <- -a/b
arrows(2.5 , 2, x2, 0,angle = 20)
text(2.5, 2.1 , "x2 = 2")
```
::::
:::: column
```{r}
plot(x, y(x), type = "l", main = "Second iteration",
     ylab = TeX("$x^2$"))
x0 <- -2
x1 <- 2
points(x0, y(x0))
points(x1, y(x1))
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
arrows(x0, 1.7, x0, y(x0), 
       angle = 20)
text(x0, 1.5, "x1 = -2")
arrows(x1, 1.7, x1, y(x1), 
       angle = 20)
text(x1, 1.5 , "x2 = 2")

```
::::
:::

## Regula Falsi: A Motivative Example
Borrowing the idea of the Bisection Method, what if we start with two points that straddle the root? 
```{r, fig.width=7, fig.height=4.5}
y <- function(x){x^3 - 1}
x <- seq(-0.5,2.5, 0.1)
plot(x,y(x), type = "l", 
     ylab = TeX("$x^3 - 1$"),
     main = "1st Iteration")
abline(h = 0, col = "cyan", lwd = 2)
x0 <- 2
x1 <- 0
points(x0, y(x0))
points(x1, y(x1))
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
points(-a/b, 0)
x2 <- -a/b
arrows(x2, 4, x2, 0, 
       angle = 20)
text(x2, 4.2, "x2 = 0.25")
arrows(x0, 1, x0, y(x0), 
       angle = 20)
text(x0, 0.8, "x0 = 2")
arrows(x1, 2, x1, y(x1), 
       angle = 20)
text(x1, 2.5, "x1 = 0")

```

## Regula Falsi: A Motivative Example

::: columns
:::: column
```{r}
plot(x,y(x), type = "l", main = "2nd Iteration: Secant")
abline(h = 0, col = "cyan", lwd = 2)
x0 <- 0
x1 <- x2
points(x0, y(x0))
points(x1, y(x1))
arrows(x0, 2, x0, y(x0), 
       angle = 20)
text(x0, 2.5, "x1 = 0")
arrows(x1, 4, x1, y(x1), 
       angle = 20)
text(x1, 4.2, "x2 = 0.25")
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
points(-a/b, 0, pch = 19)
```
::::
:::: column
```{r}
plot(x,y(x), type = "l", main = "2nd Iteration: Regula Falsi")
abline(h = 0, col = "cyan", lwd = 2)
x0 <- 2
x1 <- x2
points(x0, y(x0))
points(x1, y(x1))
arrows(x0, 1, x0, y(x0), 
       angle = 20)
text(x0, 0.8, "x0 = 2")
arrows(x1, 4, x1, y(x1), 
       angle = 20)
text(x1, 4.2, "x2 = 0.25")
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
points(-a/b, 0, pch = 19)
```
::::
:::
\noindent In this case, since the slope of the secant used in the Secant Method is so close to 0, the root is out of our scope. \linebreak
\linebreak
However, by straddling the root, the Regula Falsi makes sure that the new root is always between the previous two values.

## Regula Falsi
A variant of the Secant Method where instead of choosing the secant through $(x_n, g(x_n))$ and $(x_{n-1}, g(x_{n-1}))$, one finds the secant through $(x_n, g(x_n))$ and $(x_{n'}, g(x_{x'}))$ where $n' < n$ is the largest index for which $g(x_n)g(x_{n'})<0$.

```{r, fig.height=4.5}
y <- function(x){x^3 - 1}
regula_falsi_method_ex <- function(FUN = y, x0, x1, e_tol = 1e-5)
{
  xn <- matrix(c(x0, x1), nrow = 1)
  fn <- matrix(c(y(x0), y(x1)), nrow = 1)
  i <-  2
  err <- Inf
  while(err > e_tol)
  {
    b <- (y(xn[i-1,1]) - y(xn[i-1,2]))/(xn[i-1,1]-xn[i-1,2])
    a <- y(xn[i-1,1]) - b * (xn[i-1,1])
    x_new <- -a/b
    y_new <- y(x_new)
    if(y(xn[i-1,1]) * y_new <0)
    {
      xn <- rbind(xn, c(xn[i-1,1], x_new))
      fn <- rbind(fn, c(y(xn[i-1,1]), y_new))
    }else{
      xn <- rbind(xn, c(x_new, xn[i-1,2]))
      fn <- rbind(fn, c(y_new, y(xn[i-1,1])))
    }
    err <- abs(y_new)
    i <- i + 1
  }
  return(list(fn = fn,
              xn = xn,
              err = err))
}
temp <- regula_falsi_method_ex(y, 2, 0)
x <- seq(0,2.01, 0.1)
plot(x,y(x), type = "l", 
     ylab = TeX("$x^3 - 1$"),
     main = "Iterations")
abline(h = 0, col = "cyan", lwd = 2)
for(i in 1 : nrow(temp$fn))
{
  segments(temp$xn[i,1], temp$fn[i,1],
           temp$xn[i,2], temp$fn[i,2],
           col = rgb(153/255, 50/255, 204/255, 
                 seq(0.2,1, length.out = nrow(temp$xn))))
}
```

## Regula Falsi: Order of Convergence
Like the Bisection Method, the Regula Falsi is "safe". However, from the previous example, we see that this method is in general a first-order method.\linebreak
Especially, if $g(x)$ is convex on $[x_0, x_1]$, then 
\[|\epsilon_{n+1}| \approx C|\epsilon_n||\epsilon_0| = C'|\epsilon_n|\] where $C = \dfrac{g''(c)}{2g'(c)}$\linebreak
The Regula Falsi Method tends to retain one end-point for several iterations. As a result, it can be a good "start" method or a part of a "hybrid" method, but it should not be used near a root. 

## Illinois Algorithm: Building on Regula Falsi
In the previous example, if we artificially create a shallower secant, then maybe the end-point will no longer be retained.
```{r, fig.height=5}
y <- function(x){x^3 - 1}
x <- seq(0,2.01, 0.1)
plot(x,y(x), type = "l", 
     ylab = TeX("$x^3 - 1$"),
     main = "")
abline(h = 0, col = "cyan", lwd = 2)
x0 <- 2
x1 <- 0
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
x1 <- -a/b
b <- (y(x1) - y(x0)/8)/(x1-x0)
a <- y(x0)/8 - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "orange",
      lty = 6)
points(x1, y(x1), pch = 19, col = "darkorchid", cex= 1.5)
points(-a/b, y(-a/b), pch = 19, col = "orange", cex = 1.5)
segments(x1, 0, 
         x1, y(x1), lty = 2)
segments(x0, y(x0),
         x0, y(x0)/8, lty = 2)
segments(-a/b, 0,
         -a/b, y(-a/b), lty = 2)
legend("topleft", legend = c("1st iter",
                             "2nd iter",
                             "0.25", "1.176"),
       col = c("darkorchid", "orange"),
       lty = c(1,6, NA,NA),pch = c(NA,NA, 19,19),
       bty = "n", cex = 1.5)

```
\noindent By dividing the function value at 2 by 8 and calculating the new secant, we find a new root on right of the root. In the next iternation, the new root 1.176 instead of 2 will be used.

## Illinois Algorithm
1. During the Regula Falsi procedure, once we find one end-point has been retained more than once, we half the function value at that point, find the secant line and the new root.
2. If the point still retains, we repeat Step 1.
3. Once the point changes, we proceed with the Regula Falsi

## Illinois Algorithm: An Example
```{r, fig.height=5.5}
y <- function(x){x^3 - 1}
illinois_ex <- function(FUN = y, x0, x1, e_tol = 1e-5)
{
  xn <- matrix(c(x0, x1), nrow = 1)
  fn <- matrix(c(y(x0), y(x1)), nrow = 1)
  err <- Inf
  i <- 2
  while(err > e_tol)
  {
    y_vec <- c(fn[i-1,1], fn[i-1,2])
    if(i > 2)
    {
      if(xn[i-1,1] == xn[i-2,1])
      {
        y_vec[1] <- y_vec[1]/2
      }else if(xn[i-1,2] == xn[i-2,2]){
        y_vec[2] <- y_vec[2]/2
      }
    }
    b <- (y_vec[1] - y_vec[2])/(xn[i-1,1]-xn[i-1,2])
    a <- y_vec[1] - b * (xn[i-1,1])
    x_new <- -a/b
    y_new <- y(x_new)
    if((y_vec[1]) * y_new < 0)
    {
      xn <- rbind(xn, c(xn[i-1,1], x_new))
      fn <- rbind(fn, c(y_vec[1], y_new))
    }else{
      xn <- rbind(xn, c(x_new, xn[i-1,2]))
      fn <- rbind(fn, c(y_new, y_vec[2]))
    }
    err <- abs(y_new)
    i <- i + 1
  }
  return(list(fn = fn,
              xn = xn,
              err = err))
}

temp <- illinois_ex(y, 2, 0)
x <- seq(0,2.01, 0.1)
plot(x,y(x), type = "l", 
     ylab = TeX("$x^3 - 1$"),
     main = "Iterations")
abline(h = 0, col = "cyan", lwd = 2)
for(i in 1 : 10)
{
  segments(temp$xn[i,1], temp$fn[i,1],
           temp$xn[i,2], temp$fn[i,2],
           col = rgb(153/255, 50/255, 204/255, 
                 seq(0.8,1, length.out = 10)))
}
```
\noindent Compared with the Regula Falsi Method, the Illinois Algorithm gets in a small neighborhood of the root in just 4 or 5 iterations.

## Illinois Algorithm: Order of Convergence
The order of convergence of the Illinois Algorithm is found based on the following two observations: \[\epsilon_{n+1} = \dfrac{g''(c)}{2g'(c)}\epsilon_n \epsilon_{n-1}\] and 
asymptotically, we will perform the Illinois step (halving the function value) once every third time. \linebreak \linebreak
The order of convergence is found to be approximately 1.44.

## Successive Parabolic Interpolation
Recall the Newton's Method for optimization can be written as \[x_{n+1} = x_n - \dfrac{f'(x_n)}{f''(x_n)}\]
The essence of Newton's Method is locally approximating a function via a sequence of parabolas.\linebreak
\linebreak
If $f'$ or $f''$ is hard to find, the Successive Parabolic Interpolation Method can be used to find the extremum.\linebreak
\linebreak
In each iteration, we fit a parabola to 3 unique points and replace the "oldest" one with the extremum of the fitted parabola.

## Successive Parabolic Interpolation: vs. Newton's Method

::: columns
:::: column
```{r}
y <- function(x){0.25*x^4 - x}
dy <- function(x){x^3 - 1}
ddy <- function(x){3*x^2}
x <- seq(0,2.01, 0.01)
plot(x, y(x), type = "l",ylab = TeX("$0.25x^4 - x$"),
     main = "Newton's 1st Iteration")
x0 <- 1.5
lines(x, y(x0) + (x - x0) * dy(x0) + 
        0.5 * ((x - x0)^2) * ddy(x0), 
      col = 'cyan')
points(x0, y(x0), cex = 2)
points(x0 - dy(x0)/ddy(x0),
       y(x0 - dy(x0)/ddy(x0)),
       pch = 19, cex = 2)
arrows(x0, y(x0), x0 - dy(x0)/ddy(x0),
       y(x0 - dy(x0)/ddy(x0)),
       angle = 20)
legend("top", legend = c("x^4/4 - x",
                             "Parabola"),
       lty = 1, col = c(1, "cyan"),
       bty = "n")
```
::::
:::: column
```{r}
y <- function(x){0.25*x^4 - x}
x <- seq(0,2.01, 0.01)
plot(x, y(x), type = "l",ylab = TeX("$0.25x^4 - x$"),
     main = "Successive Parabolic 1st Iteration")
x0 <- c(2, 1.75, 1.5)
y0 <- y(x0)
points(x0, y0, cex = 2)
reg <- lm(y0 ~ x0 + I(x0^2))
lines(x, predict(reg, data.frame(x0 = x)),
      col = "cyan")
temp <- unname(reg$coefficients)
points(-temp[2]/(2 * temp[3]), y(-temp[2]/(2 * temp[3])),
       pch = 19, cex = 2)

legend("top", legend = c("x^4/4 - x",
                             "Parabola"),
       lty = 1, col = c(1, "cyan"),
       bty = "n")
```
::::
:::
\noindent The parabola fitted in the Successive Parabolic Interpolation depends on the 3 points we chose. \linebreak
\linebreak
In the next iteration, Newton's Method will fit a parabola based on the point 1.1481.\linebreak
\linebreak
The Successive Parabolic Interpolation will fit a parabola based on 1.75, 1.5, and 1.2653.

## Successive Parabolic Interpolation: An Example

```{r, fig.height=6}
quadratic_opt_ex <- function(FUN = y, x0, 
                             x_tol = 1e-5,
                             f_tol = 1e-5,
                             max_n_iter = 1000)
{
  xn <- x0
  fn <- y(x0)
  f_err <- Inf
  x_err <- Inf
  i <- 4
  while((i < max_n_iter) & 
        (f_err > f_tol) &
        (x_err > x_tol))
  {
    x_temp <- xn[(i - 3) : (i - 1)]
    y_temp <- fn[(i - 3) : (i - 1)]
    reg <- lm(y_temp ~ x_temp + I(x_temp^2))
    coef <- unname(reg$coefficients)
    
    x_new <- -coef[2]/(2 * coef[3])
    y_new <- y(x_new)
    xn <- c(xn, x_new)
    fn <- c(fn, y_new)
    x_err <- abs(xn[i] - xn[i-1])
    f_err <- abs(fn[i] - fn[i-1])
    i <- i + 1
  }
  return(list(fn = fn,
              xn = xn,
              x_err = x_err,
              f_err = f_err))
}
y <- function(x){0.25*x^4 - x}
x <- seq(0,2.01, 0.1)
plot(x, y(x), type = "l", main = "Iterations",
     ylab = TeX("$0.25x^4 - x$"))
temp <- quadratic_opt_ex(y, c(2, 1.75, 1.5))
points(temp$xn, temp$fn, cex = 2,
       pch = 19, col = rgb(153/255, 50/255, 204/255, 
                 seq(0.1,1, length.out = length(temp$xn))))

```
\noindent The Order of Convergence of the Successive Parabolic Interpolation is approximately 1.3.

## Summary: Convergence Rates
```{r}
temp <- data.frame(matrix(NA,
                          nrow = 3, 
                          ncol = 2))
colnames(temp) <- c("Root Finding", "Optimization")
rownames(temp) <- c("Linear", "Superlinear",
                    "Quadratic")
temp[1,] <- c("Bisection, Regula Falsi", "Golden Section")
temp[2,] <- c("Secant, Illinois", "Parabolic Interpolation")
temp[3,] <- rep("Newton",2)
kable(temp, "simple")
```

# Stopping and Condition 
## Three options for termination
1. Too many steps
  + Usually indicate a serious error in problem specification
2. No change in $x$ or No change in the function values
  + Need to check if a root or an extremum is being approached 
  + Root finding: in some cases, no $x$ will produce $g(x)$ "close" to 0
  + Root finding: at some function value, there may appear to be multiple roots 
  + Both absolute change $|x_{n+1} - x_n| < \epsilon_x$ and relative change $||x_{n+1} - x_n| < |x_n|\epsilon_x|$ can be used for $x$. 
  + For $g$ only absolute change for root finding. Relative change is appropriate with optimization.
  

# Appendix (Details for order of convergence analysis)

## Newton's Method: Multivariate Case
For optimizing a function $f$ of $p$ variables $\textbf{x}$, begin with\newline\newline
$f_q(\textbf{x})=f(\textbf{x}_{old})+(\textbf{x}-\textbf{x}_{old})^T\nabla f(\textbf{x}_{old})+\frac{1}{2}(\textbf{x}-\textbf{x}_{old})^T\nabla^2f(\textbf{t}_{old})(\textbf{x}-\textbf{x}_{old})$\newline\newline
The stationary point $\textbf{x}_{new}$ of $f_q(\textbf{x})$ is then given by\newline\newline
$\textbf{x}_{new}=\textbf{x}_{old}-\textbf{H}(\textbf{x}_{old})^{-1}\nabla f(\textbf{x}_{old})$\newline\newline
Where $\textbf{H}(\textbf{x})=\nabla^2f(\textbf{x})$ is the Hessian matrix. If $\textbf{H}(\textbf{x})$ is positive(resp., negative) definite at the stationary point, then a local minimum(resp., maximum) is found; if the Hessian is indefinite then a saddle point is found.

## The Secant Method: Several Definitions
Given $n + 1$ distinct pairs $\{(x_0, g(x_0)), (x_1, g(x_1)), \dots, (x_n, g(x_n))\}$, we will define:\linebreak
\linebreak
$int(x_0, x_1, \dots, x_n)$: the smallest interval that contains $x_0, \dots, x_n$\linebreak
\linebreak
The divided differences \[g[x_0, x_1, \dots, x_j, x] = \dfrac{g[x_0, x_1, \dots, x_{j-1}, x] - g[x_0, x_1, \dots, x_j]}{x - x_j}\], and \[g[x_0, x] = \dfrac{g(x) - g(x_0)}{x - x_0}\]

## The Secant Method: Newton's Interpolation Formula
Given $n + 1$ distinct pairs $\{(x_0, g(x_0)), (x_1, g(x_1)), \dots, (x_n, g(x_n))\}$, we can interpolate these points using a polynomial $q(x)$ of degree $n$.\linebreak
Specifically, \[q(x) = g(x_0) + \sum_{j=1}^n g[x_0, x_1, \dots, x_j]\prod_{i=0}^{j-1}(x-x_i)\], with the remainder \[g(x) - q(x) = \dfrac{g^{n+1}(\xi)\prod_{i = 0}^n(x-x_i)}{(n+1)!}\], where $\xi \in int(x_0, x_1, \dots, x_n, x)$


## The Secant Method: Order of Convergence 
Let $int(x_0, x_1, \dots, x_n)$: the smallest interval that contains $x_0, \dots, x_n$\newline
According to Newton's interpolation formula, we have \[g(x) = g(x_n) + (x-x_n)g[x_{n-1},x_n]+\dfrac{1}{2}(x-x_n)(x-x_{n-1})g''(\xi)\] where $g[x_{n-1},x_n] = \dfrac{g(x_n)-g(x_{n-1})}{x_n-x_{n-1}}$, and $\xi \in int(x, x_n, x_{n-1})$\linebreak
\linebreak
By the Secant Method, we have $x_{n+1} = x_{n} - g(x_n)\dfrac{x_{n} - x_{n-1}}{g(x_n) - g(x_{n-1})}\Rightarrow \linebreak \linebreak
0 = g(x_n)+(x_{n+1}-x_n)g[x_{n-1},x_n]$

## The Secant Method: Order of Convergence 
Let the root the Secant Method approaches be $c$, then
\[0=g(c)-g(x_n)-(x_{n+1}-x_n)g[x_{n-1},x_n] = \]
\[g[x_{n-1},x_n](c-x_{n+1}) + \dfrac{1}{2}(c-x_n)(c-x_{n-1})g''(\xi)\]
By the mean value theorem, we have 
\[g[x_{n-1},x_n] = g'(\eta), \eta\in (x_{n-1},x_n)\]
Let $\epsilon_n = c - x_n$, we get $0=g'(\eta)\epsilon_{n+1} + \dfrac{1}{2}\epsilon_n \epsilon_{n-1}g''(\xi)\Rightarrow \linebreak 
\epsilon_{n+1} = \dfrac{g''(\xi)}{2g'(\eta)}\epsilon_n \epsilon_{n-1}$

## The Secant Method: Order of Convergence 
Now suppose the Secant Method converges, then when $n\rightarrow \infty$, $\xi \approx c$ and $\eta \approx c$. Let $C = |\dfrac{g''(c)}{2g'(c)}|$, then $|\epsilon_{n+1}| = C|\epsilon_n| |\epsilon_{n-1}|$\linebreak\linebreak
To find the order of convergence, we find $M$ and $p$ such that $|\epsilon_{n+1}|\approx M|\epsilon_n|^p$, where $p$ is the convergence order.\linebreak
$M|\epsilon_n|^p = C(M|\epsilon_{n-1}|^{p})|\epsilon_{n-1}| \Rightarrow 
|\epsilon_n| = C|\epsilon_{n-1}|^{(1+p)/p}$\linebreak
This implies $p = (1+p)/p \Rightarrow p = 1+\phi \approx 1.618$\linebreak\linebreak
Since the exponent 1.618 lies between 1 (linear convergence) and 2 (quadratic convergence), the convergence rate of the Secant Method is called \emph{superlinear}.

## Illinois Algorithm: Order of Convergence
Recall the errors of the Secant Method satisfy: $\epsilon_{n+1} = \dfrac{g''(\xi)}{2g'(\eta)}\epsilon_n \epsilon_{n-1}$.\linebreak
If $g''$ is continuous, then when the Illinois Algorithm gets into a sufficient small neighborhood of the root $c$, we can assume $g'$ and $g''$ have constant sign.\linebreak
This implies that $\dfrac{\epsilon_{n+1}}{\epsilon_n \epsilon_{n-1}}$ also has constant sign. \linebreak
Since $g_{n-1}g_n < 0 \Rightarrow \epsilon_{n-1}\epsilon_n <0$, we then necessarily have the sign of $\epsilon_n$'s follow one of the two schemes:
\[\dots + - + + - + + - + \dots\] 
or \[\dots +--+--+--\dots\]

## Illinois Algorithm: Order of Convergence
Previous analysis shows that asymptotically, an end-point will be retained twice in consecutive three iterations.\linebreak
In other words, we will perform the Illinois step (halving the function value) once every third time. \linebreak
Further asymptotic analysis shows that an Illinois step has $\epsilon_{n+1}\approx -\epsilon_n$ \linebreak
Putting the pieces together, we have 
\[\epsilon_{n} = -\epsilon_{n-1}\Rightarrow \epsilon_{n+1} = -C\epsilon_{n-1}^2 \Rightarrow \epsilon_{n+2} = C^2\epsilon_{n-1}^3\]
Via finding $p$ such that $|\epsilon_n| = M|\epsilon_{n-1}|^p$, we get $\dfrac{3}{p^2} = p \Rightarrow p \approx 1.44$

