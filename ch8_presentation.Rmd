---
title: "Introduction to Optimization and Nonlinear Equations"
author: "Zeyu Lu & Yuqiu Yang"
header-includes:
  - \usepackage{subfig}
output: 
  beamer_presentation:
    theme: "Berkeley"
    colortheme: "dolphin"
    fonttheme: "structurebold"
    toc: true
    slide_level: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(latex2exp)
library(knitr)
library(kableExtra)
library(gridExtra)
library(grid)
```

# Safe Univariate Methods:

## Lattice Search




## Golden Search



## Bisection

# Root finding 

## Newton's Method 


## The Secant Method 
If $g'$ is hard or even impossible to find, we can approximate $g'(x) \approx \dfrac{g(x+h)-g(x)}{h}$. \linebreak
\linebreak
The iteration formula now becomes $x_{n+1} = x_{n} - g(x_n)\dfrac{x_{n} - x_{n-1}}{g(x_n) - g(x_{n-1})}$\linebreak
\linebreak
Notice two initial approximations are required instead of one like the Newton's method.

## The Secant Method: Geometrical Interpretation
Let $f(x) = sin(x) - (\dfrac{x}{2})^2$, $x_0 = -1.5$ and $x_{1} = 1.5$
```{r, fig.align='center', out.width='70%', echo=FALSE}
x <- seq(-2, 3, by = 0.01)
y <- function(x){sin(x) - x^2/4}
dy <- function(x) {cos(x) - x/2}
plot(x, y(x), type = 'l',
     main = "1st iteration Newton vs. Secant")
abline(h = 0, col = "cyan", lwd = 2)
x0 <- -1.5
b <- dy(x0)
a <- y(x0) - b * (x0)
newton_line <-  b * x + a
lines(x, newton_line, col = "salmon")
points(-a/b, 0, pch = 19, cex = 2, 
       col = "salmon")
x1 <- 1.5
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
points(-a/b, 0, pch = 19, cex = 2, 
       col = "darkorchid")
points(x0, y(x0))
points(x1, y(x1))
arrows(x0, -1, x0, y(x0), 
       angle = 20)
text(x0, -0.8, "x0 = -1.5")
arrows(x1, -0.1, x1, y(x1), 
       angle = 20)
text(x1, -0.3 , "x1 = 1.5")
legend("bottom", legend = c("Newton", "Secant",
                            "Newton 1st iter",
                            "Secant 1st iter"),
       col = c("salmon", "darkorchid"),
       lty = c(1,1,NA,NA),
       pch = c(NA,NA,19,19),
       bty = "n")
```
\noindent $x_{n + 1}$ is taken to be the abscissa of the point of intersection between the secant through $(x_{n-1}, f(x_{n-1}))$ and $(x_{n}, f(x_{n}))$ and the x-axis.

## The Secant Method: An Example
::: columns
:::: column
Let $f(x) = sin(x) - (\dfrac{x}{2})^2$\linebreak
$x_0 = -1.5$ and $x_{1} = 1.5$\linebreak 
```{r, echo=FALSE, fig.height=11}
x <- seq(-2, 3, by = 0.01)
y <- function(x){sin(x) - x^2/4}
plot(x, y(x), type = 'l',
     main = "Secant method")
abline(h = 0, col = "cyan", lwd = 2)
secant_method_ex <- function(FUN = y, x0, x1, e_tol = 1e-5)
{
  xn <- c(x0, x1)
  fn <- c(y(x0), y(x1))
  i <-  3
  err <- Inf
  while(err > e_tol)
  {
    b <- (y(xn[i-1]) - y(xn[i-2]))/(xn[i-1]-xn[i-2])
    a <- y(xn[i-2]) - b * (xn[i-2])
    xn <- c(xn, -a/b)
    fn <- c(fn, y(-a/b))
    err <- abs(y(-a/b))
    i <- i + 1
  }
  return(list(fn = fn,
              xn = xn,
              err = err))
}
temp <- secant_method_ex(y, -1.5,1.5)
points(temp$xn, temp$fn, pch = 19, cex =2.5,
       col = rgb(153/255, 50/255, 204/255, 
                 seq(0.1,1, length.out = length(temp$xn))))
```
::::
:::: column
```{r}
######
temp_df <- data.frame(matrix(NA, nrow = length(temp$fn),
                             ncol = 3))
colnames(temp_df) <- c("n", "xn", "fn")
temp_df$n <- 0:11
temp_df$xn <- temp$xn
temp_df$fn <- temp$fn
knitr::kable(temp_df, "simple")

```
::::
:::

## The Secant Method: Several Definitions 
Given $n + 1$ distinct pairs $\{(x_0, g(x_0)), (x_1, g(x_1)), \dots, (x_n, g(x_n))\}$, we will define:\linebreak
\linebreak
$int(x_0, x_1, \dots, x_n)$: the smallest interval that contains $x_0, \dots, x_n$\linebreak
\linebreak
The divided differences \[g[x_0, x_1, \dots, x_j, x] = \dfrac{g[x_0, x_1, \dots, x_{j-1}, x] - g[x_0, x_1, \dots, x_j]}{x - x_j}\], and \[g[x_0, x] = \dfrac{g(x) - g(x_0)}{x - x_0}\]

## The Secant Method: Newton's Interpolation Formula 
Given $n + 1$ distinct pairs $\{(x_0, g(x_0)), (x_1, g(x_1)), \dots, (x_n, g(x_n))\}$, we can interpolate these points using a polynomial $q(x)$ of degree $n$.\linebreak
Specifically, \[q(x) = g(x_0) + \sum_{j=1}^n g[x_0, x_1, \dots, x_j]\prod_{i=0}^{j-1}(x-x_i)\], with the remainder \[g(x) - q(x) = \dfrac{g^{n+1}(\xi)\prod_{i = 0}^n(x-x_i)}{(n+1)!}\], where $\xi \in int(x_0, x_1, \dots, x_n, x)$

## The Secant Method: Order of convergence 
According to Newton's interpolation formula, we have \[g(x) = g(x_n) + (x-x_n)g[x_{n-1},x_n]+\dfrac{1}{2}(x-x_n)(x-x_{n-1})g''(\xi)\] where $g[x_{n-1},x_n] = \dfrac{g(x_n)-g(x_{n-1})}{x_n-x_{n-1}}$, and $\xi \in int(x, x_n, x_{n-1})$\linebreak
\linebreak
By the Secant Method, we have $x_{n+1} = x_{n} - g(x_n)\dfrac{x_{n} - x_{n-1}}{g(x_n) - g(x_{n-1})}\Rightarrow \linebreak \linebreak
0 = g(x_n)+(x_{n+1}-x_n)g[x_{n-1},x_n]$

## The Secant Method: Order of convergence 
Let the root the Secant Method approaches be $c$, then
\[0=g(c)-g(x_n)+(x_{n+1}-x_n)g[x_{n-1},x_n] = \]
\[g[x_{n-1},x_n](c-x_{n+1}) + \dfrac{1}{2}(c-x_n)(c-x_{n-1})g''(\xi)\]
By the mean value theorem, we have 
\[g[x_{n-1},x_n] = g'(\eta), \eta\in (x_{n-1},x_n)\]
Let $\epsilon_n = c - x_n$, we get $0=g'(\eta)\epsilon_{n+1} + \dfrac{1}{2}\epsilon_n \epsilon_{n-1}g''(\xi)\Rightarrow \linebreak 
\epsilon_{n+1} = \dfrac{g''(\xi)}{2g'(\eta)}\epsilon_n \epsilon_{n-1}$

## The Secant Method: Order of convergence 
Now suppose the Secant Method converges, then when $n\rightarrow \infty$, $\xi \approx c$ and $\eta \approx c$. Let $C = \dfrac{g''(c)}{2g'(c)}$, then $|\epsilon_{n+1}| = C|\epsilon_n| |\epsilon_{n-1}|$\linebreak\linebreak
To find the order of convergence, we find $p$ such that $|\epsilon_{n+1}|\approx M|\epsilon_n|^p \Rightarrow \linebreak
M|\epsilon_n|^p = MM|\epsilon_{n-1}|^{p}|\epsilon_{n-1}| \Rightarrow 
|\epsilon_n| = M|\epsilon_{n-1}|^{(1+p)/p}$\linebreak
This implies $p = (1+p)/p \Rightarrow p = 1+\phi \approx 1.618$\linebreak\linebreak
Since the exponent 1.618 lies between 1 (linear convergence) and 2 (quadratic convergence), the convergence rate of the Secant Method is called \emph{superlinear}.

## The Secant Method: Pros and Cons
1. Pros:
  + Superlinear convergence 
  + No need to evaluate derivatives 
2. Cons:
  + Convergence is not guaranteed 
  + Not well behaved when $g$ is relatively flat

::: columns
:::: column
```{r}
y <- function(x){x^2}
x <- seq(-3,3,by = 0.01)
plot(x, y(x), type = "l", main = "First iteration")
x0 <- 1
x1 <- -2
points(x0, y(x0))
points(x1, y(x1))
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
arrows(x0, 4, x0, y(x0), 
       angle = 20)
text(x0, 4.2, "x0 = 1")
arrows(x1, 1.7, x1, y(x1), 
       angle = 20)
text(x1, 1.5 , "x1 = -2")
abline(h = 0, col = "cyan", lwd = 2)
points(-a/b, 0)
x2 <- -a/b
arrows(2.5 , 2, x2, 0,angle = 20)
text(2.5, 2.1 , "x2 = 2")
```
::::
:::: column
```{r}
plot(x, y(x), type = "l", main = "Second iteration")
x0 <- -2
x1 <- 2
points(x0, y(x0))
points(x1, y(x1))
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
arrows(x0, 1.7, x0, y(x0), 
       angle = 20)
text(x0, 1.5, "x1 = -2")
arrows(x1, 1.7, x1, y(x1), 
       angle = 20)
text(x1, 1.5 , "x2 = 2")

```
::::
:::

## Regula Falsi: A Motivative Example
Borrowing the idea of the Bisection Method, what if we start with two points that straddle the root? 
```{r, fig.width=7, fig.height=4.5}
y <- function(x){x^3 - 1}
x <- seq(-0.5,2.5, 0.1)
plot(x,y(x), type = "l", 
     ylab = TeX("$x^3 - 1$"),
     main = "1st Iteration")
abline(h = 0, col = "cyan", lwd = 2)
x0 <- 2
x1 <- 0
points(x0, y(x0))
points(x1, y(x1))
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
points(-a/b, 0)
x2 <- -a/b
arrows(x2, 4, x2, 0, 
       angle = 20)
text(x2, 4.2, "x2 = 0.25")
arrows(x0, 1, x0, y(x0), 
       angle = 20)
text(x0, 0.8, "x0 = 2")
arrows(x1, 2, x1, y(x1), 
       angle = 20)
text(x1, 2.5, "x1 = 0")

```

## Regula Falsi: A Motivative Example

::: columns
:::: column
```{r}
plot(x,y(x), type = "l", main = "2nd Iteration: Secant")
abline(h = 0, col = "cyan", lwd = 2)
x0 <- 0
x1 <- x2
points(x0, y(x0))
points(x1, y(x1))
arrows(x0, 2, x0, y(x0), 
       angle = 20)
text(x0, 2.5, "x1 = 0")
arrows(x1, 4, x1, y(x1), 
       angle = 20)
text(x1, 4.2, "x2 = 0.25")
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
points(-a/b, 0, pch = 19)
```
::::
:::: column
```{r}
plot(x,y(x), type = "l", main = "2nd Iteration: Regula Falsi")
abline(h = 0, col = "cyan", lwd = 2)
x0 <- 2
x1 <- x2
points(x0, y(x0))
points(x1, y(x1))
arrows(x0, 1, x0, y(x0), 
       angle = 20)
text(x0, 0.8, "x0 = 2")
arrows(x1, 4, x1, y(x1), 
       angle = 20)
text(x1, 4.2, "x2 = 0.25")
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
points(-a/b, 0, pch = 19)
```
::::
:::

## Regula Falsi
A variant of the Secant Method where instead of choosing the secant through $(x_n, g(x_n))$ and $(x_{n-1}, g(x_{n-1}))$, one finds the secant through $(x_n, g(x_n))$ and $(x_{n'}, g(x_{x'}))$ where $n' < n$ is the largest index for which $g(x_n)g(x_{n'})<0$.

```{r, fig.height=4.5}
y <- function(x){x^3 - 1}
regula_falsi_method_ex <- function(FUN = y, x0, x1, e_tol = 1e-5)
{
  xn <- matrix(c(x0, x1), nrow = 1)
  fn <- matrix(c(y(x0), y(x1)), nrow = 1)
  i <-  2
  err <- Inf
  while(err > e_tol)
  {
    b <- (y(xn[i-1,1]) - y(xn[i-1,2]))/(xn[i-1,1]-xn[i-1,2])
    a <- y(xn[i-1,1]) - b * (xn[i-1,1])
    x_new <- -a/b
    y_new <- y(x_new)
    if(y(xn[i-1,1]) * y_new <0)
    {
      xn <- rbind(xn, c(xn[i-1,1], x_new))
      fn <- rbind(fn, c(y(xn[i-1,1]), y_new))
    }else{
      xn <- rbind(xn, c(x_new, xn[i-1,2]))
      fn <- rbind(fn, c(y_new, y(xn[i-1,1])))
    }
    err <- abs(y_new)
    i <- i + 1
  }
  return(list(fn = fn,
              xn = xn,
              err = err))
}
temp <- regula_falsi_method_ex(y, 2, 0)
x <- seq(0,2.01, 0.1)
plot(x,y(x), type = "l", 
     ylab = TeX("$x^3 - 1$"),
     main = "Iterations")
abline(h = 0, col = "cyan", lwd = 2)
for(i in 1 : nrow(temp$fn))
{
  segments(temp$xn[i,1], temp$fn[i,1],
           temp$xn[i,2], temp$fn[i,2],
           col = rgb(153/255, 50/255, 204/255, 
                 seq(0.2,1, length.out = nrow(temp$xn))))
}
```

## Regula Falsi: Order of Convergence
Like the Bisection Method, the Regula Falsi is "safe". However, from the previous example, we see that this method is in general a first-order method.\linebreak
Especially, if $g(x)$ is convex on $[x_0, x_1]$, then 
\[|\epsilon_{n+1}| \approx C|\epsilon_n||\epsilon_0| = C'|\epsilon_n|\] where $C = \dfrac{g''(c)}{2g'(c)}$\linebreak
The Regula Falsi Method tends to retain one end-point for several iterations. As a result, it can be a good "start" method or a part of a "hybrid" method, but it should not be used near a root. 

## Illinois Algorithm: Building on Regula Falsi
In the previous example, if we artificially create a shallower secant, then maybe the end-point will no longer be retained.
```{r, fig.height=5}
y <- function(x){x^3 - 1}
x <- seq(0,2.01, 0.1)
plot(x,y(x), type = "l", 
     ylab = TeX("$x^3 - 1$"),
     main = "")
abline(h = 0, col = "cyan", lwd = 2)
x0 <- 2
x1 <- 0
b <- (y(x1) - y(x0))/(x1-x0)
a <- y(x0) - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "darkorchid")
x1 <- -a/b
b <- (y(x1) - y(x0)/8)/(x1-x0)
a <- y(x0)/8 - b * (x0)
secant_line <- b * x + a
lines(x, secant_line, col = "orange",
      lty = 6)
points(x1, y(x1), pch = 19, col = "darkorchid", cex= 1.5)
points(-a/b, y(-a/b), pch = 19, col = "orange", cex = 1.5)
segments(x1, 0, 
         x1, y(x1), lty = 2)
segments(x0, y(x0),
         x0, y(x0)/8, lty = 2)
segments(-a/b, 0,
         -a/b, y(-a/b), lty = 2)
legend("topleft", legend = c("1st iter",
                             "2nd iter",
                             "0.25", "1.176"),
       col = c("darkorchid", "orange"),
       lty = c(1,6, NA,NA),pch = c(NA,NA, 19,19),
       bty = "n", cex = 1.5)

```
\noindent By dividing the function value at 2 by 8 and calculating the new secant, we find a new root on right of the root. In the next iternation, the new root 1.176 instead of 2 will be used.

## Illinois Algorithm
1. During the Regula Falsi procedure, once we find one end-point has been retained more than once, we half the function value at that point, find the secant line and the new root.
2. If the point still retains, we repeat Step 1.
3. Once the point changes, we proceed with the Regula Falsi

## Illinois Algorithm: An Example
```{r, fig.height=5.5}
y <- function(x){x^3 - 1}
illinois_ex <- function(FUN = y, x0, x1, e_tol = 1e-5)
{
  xn <- matrix(c(x0, x1), nrow = 1)
  fn <- matrix(c(y(x0), y(x1)), nrow = 1)
  err <- Inf
  i <- 2
  while(err > e_tol)
  {
    y_vec <- c(fn[i-1,1], fn[i-1,2])
    if(i > 2)
    {
      if(xn[i-1,1] == xn[i-2,1])
      {
        y_vec[1] <- y_vec[1]/2
      }else if(xn[i-1,2] == xn[i-2,2]){
        y_vec[2] <- y_vec[2]/2
      }
    }
    b <- (y_vec[1] - y_vec[2])/(xn[i-1,1]-xn[i-1,2])
    a <- y_vec[1] - b * (xn[i-1,1])
    x_new <- -a/b
    y_new <- y(x_new)
    if((y_vec[1]) * y_new < 0)
    {
      xn <- rbind(xn, c(xn[i-1,1], x_new))
      fn <- rbind(fn, c(y_vec[1], y_new))
    }else{
      xn <- rbind(xn, c(x_new, xn[i-1,2]))
      fn <- rbind(fn, c(y_new, y_vec[2]))
    }
    err <- abs(y_new)
    i <- i + 1
  }
  return(list(fn = fn,
              xn = xn,
              err = err))
}

temp <- illinois_ex(y, 2, 0)
x <- seq(0,2.01, 0.1)
plot(x,y(x), type = "l", 
     ylab = TeX("$x^3 - 1$"),
     main = "Iterations")
abline(h = 0, col = "cyan", lwd = 2)
for(i in 1 : 10)
{
  segments(temp$xn[i,1], temp$fn[i,1],
           temp$xn[i,2], temp$fn[i,2],
           col = rgb(153/255, 50/255, 204/255, 
                 seq(0.8,1, length.out = 10)))
}
```
\noindent Compared with the Regula Falsi Method, the Illinois Algorithm gets in a small neighborhood of the root in just 4 or 5 iterations.

## Illinois Algorithm: Order of Convergence





## Optimization 

```{r}
quadratic_opt_ex <- function(FUN = y, x0, 
                             x_tol = 1e-5,
                             f_tol = 1e-5,
                             max_n_iter = 1000)
{
  xn <- x0
  fn <- y(x0)
  f_err <- Inf
  x_err <- Inf
  i <- 4
  while((i < max_n_iter) & 
        (f_err > f_tol) &
        (x_err > x_tol))
  {
    x_temp <- xn[(i - 3) : (i - 1)]
    y_temp <- fn[(i - 3) : (i - 1)]
    reg <- lm(y_temp ~ x_temp + I(x_temp^2))
    coef <- unname(reg$coefficients)
    
    x_new <- -coef[2]/(2 * coef[3])
    y_new <- y(x_new)
    xn <- c(xn, x_new)
    fn <- c(fn, y_new)
    x_err <- abs(xn[i] - xn[i-1])
    f_err <- abs(fn[i] - fn[i-1])
    i <- i + 1
  }
  return(list(fn = fn,
              xn = xn,
              x_err = x_err,
              f_err = f_err))
}
y <- function(x){0.25*x^4 - x}
x <- seq(0,2.01, 0.1)
plot(x, y(x), type = "l")
temp <- quadratic_opt_ex(y, c(0,0.1,0.2))
points(temp$xn, temp$fn, 
       pch = 19, col = rgb(153/255, 50/255, 204/255, 
                 seq(0.1,1, length.out = length(temp$xn))))

```


# Stopping and Condition 

